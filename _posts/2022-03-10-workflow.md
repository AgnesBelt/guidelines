---
layout: post
title:  "Workflow"
date:   2022-03-18
author: Nandi Moksnes
category: practice
tags: repeatability reusability reconstructability auditability
---

A computational workflow combines data and its operations of the data. Several processes are coordinated to handle dependencies between different computational steps. Workflows looks at methods which are used for (some or all of) the following:
- data collection,
- preparation, analysis and predictive modelling, and
- simulation of data ([Gobel et al (2020)][1]).

A tool or a model can be considered a computational  workflow as they can support the data analysis.

![Workflow](https://user-images.githubusercontent.com/30128518/157625071-57a8a629-8319-4168-bc57-f92c4851b2ba.png)

Creating a workflow can be divided into three stages.
1. You need to identify and describe the process and steps needed to characterise the relationship between the data inputs and outputs. Mapping the required formats and data for potential project based database or visualisation is important to reach the desired final product.
2. When developing the workflow
    - the scripts and codes that are produced needs to be easily found,
    - it needs to be clearly indicated in which sequence the scripts and codes are to be executed.
    Here, several options are possible to support and facilitate the work:
    - For generic computational workflows, [Snakemake](https://snakemake.readthedocs.io/en/stable/) is a workflow management systems tool which can handle different programs. It executes the required steps ("rules") through a Snakefile until the final output is obtained. A snakemake workflow is scalable to server, cluster, grid and cloud environment without needing to modify the snakefile;
    - If the computational workflow is defined using the Python programming language, a [Jupyter Notebook](https://jupyter.org/) (which allows for inserting comments along the script or code) can be used to describe the steps, execute functions and plot some of the results.
    - A even more simple, but still useful way, of documenting the workflow is to use add a document/.txt file with predefined set of metadata/information that describes the workflow.
3.  It is important to clearly define the different steps that need to be executed, so that the workflow can be easily implemented with tests to check that it is working properly. This will help to identify bugs and issues that could arise later on. If the code is released then it can also be deposited to [Zenodo](https://zenodo.org/) to assign a DOI to that specific release.


## Reproducible and FAIR

 When developing computational workflows your work become more reusable and reproducible for others and this in turn can help increasing collaboration. Well defined workflows contribute to the [FAIR data](https://www.go-fair.org/fair-principles/) as they provide traceability of the results and data used to produce an output. If the workflow is complex then also time and energy can be saved by creating a workflow.

## Example of collaborative workflow

The [PyPSA-Eur: An Open Optimisation Model of the European Transmission System](https://github.com/PyPSA/pypsa-eur) is a good example of how to work on collaborative workflows. The workflow is written in Python and executed using [snakemake](https://snakemake.readthedocs.io/en/stable/). Each release is deposited on Zenodo and ascribed a DOI. The documentation is available on [ReadTheDocs](https://pypsa.readthedocs.io/en/latest/) and there is also guidance that helps users identify if the available data is suitable for their needs, together with links to find more information. Furthermore, is each release documented in a academic short paper and archived as a pre-print in arxiv.

## Useful resources
[The Turing Way](https://the-turing-way.netlify.app/reproducible-research/reproducible-research.html) handbook for reproducible, ethical and collaborative data science has a chapter covering workflows [Make](https://the-turing-way.netlify.app/reproducible-research/make.html) which can be an additional useful resource.

------------
*This material is derived from the [CCG review of good enough practices][2], released under a [CC-BY 4.0 license][3].*

[1]: <https://direct.mit.edu/dint/article/2/1-2/108/10003/FAIR-Computational-Workflows> "Goble, C., Cohen-Boulakia, S., Soiland-Reyes, S., Garijo, D., Gil, Y., Crusoe, M.R., Peters, K., Schober, D., 2020. FAIR Computational Workflows, *Data Intelligence*, vol. 2, no. 1–2, 1135 pp. 108–121. DOI: 10.1162/dint_a_00033."
[2]: https://doi.org/10.5281/zenodo.5911546 "Usher, William, Beltramo, Agnese, Gardumi, Francesco, Martin, Viktoria, & Petrarulo, Luca. (2022). CCG Platform - Body of Knowledge: Review of Good Practice (1.3). Zenodo. https://doi.org/10.5281/zenodo.5911546"
[3]: https://creativecommons.org/licenses/by/4.0/legalcode
