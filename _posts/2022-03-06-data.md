---
layout: post
title:  "Data"
date:   2022-03-06 
author: Agnese Beltramo
category: Open Science
tags: metadata, referencing, attribution, release, licensing, version control, workflow
---



Data represent the basis for any research output.
Data can assume a wide range of formats: from numerical data, to text, to audio, to images.  
Similarly, data file can consist of a wide range of file formats: from comma-separated values (.csv) files, to text (.txt) files, to image (.jpeg or .png) files.

In order to support the wide reusability of data across different research projects, contexts, and applications, it is important to ensure that data file formats are as generic as possible, i.e. non software specific.

To ensure data are corectly used and managed across research projects, the following [process cycle][1] should be kept in mind:
![DataCycle](img/DataCycle.png)

### Planning:
A plan should be establish in the form of a live document, to identify types of data and related formats that are expected to be used in the project.

### Ethical and Legal Compliance:
If either personal or sensitive data, or confidential data, are expected to be collected along the project, special attention should be given to ensure that legal and ethical issues are addressed in accordance with relevant regulations. This might include, ensure that data are saved on a secure platform, and that data are released (if at all) in an anonymous and secure way. More information available under [Licensing](#licensing).

### Organizing and Documenting: 
Documents and data need to be organized in a way that can be understood and taken up by future colleagues or researchers interested in the existing data and work done. 

In order to do so, once the data start being collected, several steps can be followed to betted organize and document the process:

- *Add a [license](#licensing) and [readme file](https://www.makeareadme.com/)* to a folder containing the data file(s) of relevance.

- *Convert the data file(s) into datapackage* by providing all the necessary information to ensure original data sources, data processing steps and contributors are clearly mentioned. 

- *Add [metadata](#metadata) to the datapackage*

    >The [Data Package Creator](https://create.frictionlessdata.io/) provides easy-to-use templates to be filled in with pertaing information to manually generate a data package and related metadata.

- *Create a [workflow](#workflow)* to faclitate with simple steps and commands the reproduction of exactly the same data management process used produce the datapackage. 

- *Incorporate version control* by using a distributed [version control](#version-control) system such as [Git](https://git-scm.com/).

### Storing and Sharing: 
As briefly mentioned already in the previous step, datapackages need to be stored on a safe platform, and possibly in a way that can be shared across collaborators and retrieved by the interested audiences. 
There exist several options to publish datapackages online, either on online open data repositories, which provide open access and the possibility to retrieve the datapackages easily, or on private cloud services (e.g. Microsoft Teams, Google Drive, Dorpbox), which provide selected access to the files. 
In the case of personal or confidential data, dedicated private network drives might be recommended instead, as the povide limited access to the data and are not retreivable online.

### Releasing: 
Once a new dataset or data package has been developed, it can be released and published online, and a related Digital Object Iidentifier (DOI) can be generated. This facilitates other people in citing and making use of it as new resource or to verify and reproduce existing research that already made use of the dataset.
In order to do so, few alterrnatives exist, listed here by increasing level of complexity:

- *Manually uploading data packages and providing related metadata on an online repository*, such as [Zenodo](https://zenodo.org/), following pre-defined templates that help providing relevant [metadata](#metadata) for the release.

- *Saving the data and documenting the related processing steps on a online development platform*, such as [GitHub](https://github.com/). In this case, metadata and documentation can be embedded along the data management process, thanks to the [version control](#version-control) system.

## Metadata
Metadata need to accompany the data to provide a basic set of information meant to support the retrievability of specific outputs and their correct citation. 
Metadata for data packages or datasets must include the following:

- [Attributions](#attribution) (who the authors/contributors are, who did the work on collecting and compiling the data)
- [Licensing](#licensing) clarifying under what conditions/rules the data can be used/modified/shared, etc.

In addition, metadata should also include the following:
- [Referencing](#referencing) to source material, to provide information regarding where the data come from.
- Other relevant information, which might describe the methodology used for collecting the data, or the data format provided, or additional documentationi that might be considered relevant to understand the data and their potential use.

For more information, see the [Metadata practice]()

## Referencing
It is important to ensure that original data sources are properly documented and cited, in reference to the data used and processed in the develpent of a derivated data package or dataset used for research. 
This is needed to ensure that data are traceable.
Original data sources should be recorded and complete citations for each of the sources used in the generation of a data package or dataset should be listed in the related metadata.

For more information, see the [Referencing practice]()

## Attribution
To ensure all the contributors - i.e. people involved in collecting, processing, structuring and releaseing the data - are acknolwegded, the [CRediT author statement](https://www.elsevier.com/authors/policies-and-guidelines/credit-author-statement) can be used as reference.

For each contributors, the following basic personal information should be provided:
- Name and Surname
- Affiliation
- [ORCiD](https://orcid.org/) code
- Information about the contribution provided or the role covered by the person in question.

For more information, see the [Attribution practice]()

## Release
Releases can be created once a final version of the data as intended is produced and used for a specific application or as project output. 

Releases should be provided with a Digital Object Identifier (DOI), a complete Metadata file containing information regarding contributors and citation format.

By uploading the final dataset or data package on the [Zenodo](https://zenodo.org/) online repository, a unique DOI is generated and a template if provided to guide in the compilation of relevant [metadata](#metadata) for a release.

For more information, see the [Release practice]()

## Licensing
It is important to ensure that data are always associated with a license, to clarify what are the legal restrictions (if present) in using, sharing and/or modifying existing data.
[CC BY licences](https://creativecommons.org/about/cclicenses/) are typically used for data licensing. Among CC BY licenses, there are more permissive and more restrictive licenses. Available on the Creative Commons website, [this tool](https://creativecommons.org/choose/) can help in choosing the right license for your data.

All licenses that apply to the original data sources should also be considered in selecting the license for the derivated data package or dataset released or shared as reseach output. This is to make sure the derivated work complies with any potential legal restriction imposed on the original data sources.

For more information, see the [Licensing practice]()

## Version control
Version control systems, such as [Git](https://git-scm.com/), and dedicated online services, such as [GitHub](https://github.com/), allow to record instances of text-based material and (e.g. data saved as .csv or .txt files) and then compare them to visualize differences and changes. They also enable collaborative workflows, thus facilitating researcher to work on the same data simultaneously, save and compare the separate progresses later on.
Using the [Git](https://git-scm.com/) distributed [version control](#version-control) system, helps keeping track of changes and modifications from the original data to the final data format and structure needed, and to document each and every step live while working on the data. [GitHub](https://github.com/) can facilitate the use of [Git](https://git-scm.com/) across a large research group, thus potentially facilitating collaborations.

For more information, see the [Version Control practice]()

## Workflow
A workflow is defined by [Goble et al. (2020)][2] as the description of “...the complex multi-step process to coordinate multiple tasks and their data dependencies”, where each step “represents the execution of a computational process”.

Workflows help with executing, reusing and reproducing, as well as reporting all steps needed to characterise the processing of data, thus supporting the use of data in the research context. 

The [Snakemake](https://snakemake.readthedocs.io/en/stable/) workflow management system can help with defining and automating a workflow. This allows other researchers or stakeholders to reproduce it with fewer, simpler steps that the ones required to initially define the workflow.

For more information, see the [Workflow practice]()



# Useful resources
- [**Research Data Management**](https://the-turing-way.netlify.app/reproducible-research/rdm.html) section, from [*The Turing Way*](https://the-turing-way.netlify.app/welcome.html) handbook to reproducible, ethical and collaborative data science.
- [**Data standards for enegry modelling for policy support**](https://forum.u4ria.org/t/data-standards-for-energy-modeling-for-policy-support/25) entry, from the [*u4RIA Modelling for Policy Support*](https://forum.u4ria.org/) forum.



[1]: https://docs.google.com/presentation/d/1nujmtGu6SpQg_dn918AXTgwok6kCynY_/edit#slide=id.p4 "Viitanen E., 2022. Introduction to Research Data Management(RDM). *Aalto University*, Finland: Aalto. Available at: https://www.aalto.fi/en/services/training-in-research-data-management-and-open-science. (accessed: March 07, 2022)"

[2]: https://direct.mit.edu/dint/article/2/1-2/108/10003/FAIR-Computational-Workflows "Goble, C., Cohen-Boulakia, S., Soiland-Reyes, S., Garijo, D., Gil, Y., Crusoe, M.R., Peters, K., Schober, D., 2020. FAIR Computational Workflows, *Data Intelligence*, vol. 2, no. 1–2, 1135 pp. 108–121. DOI: 10.1162/dint_a_00033."

