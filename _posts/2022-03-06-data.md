---
layout: post
title:  "Data"
date:   2022-03-06 
author: Agnese Beltramo
category: Open Science
tags: metadata, referencing, attribution, release, licensing, version control, workflow
---

Data represent the basis for any research output.
Data can assume a wide range of formats: from numerical data, to text, to audio, to images.  
Similarly, data file can consist of a wide range of file formats: from comma-separated values (.csv) files, to text (.txt) files, to image (.jpeg or .png) files.

In order to support the wide reusability of data across different research projects, contexts, and applications, it is important to ensure that data file formats are as generic as possible, i.e. non software specific.

To ensure data are corectly used and managed across research projects, the following process cycle[ˆ1] should be kept in mind:
![DataCycle](img/DataCycle.png)

1. **Planning**: a plan should be establish in the form of a live document, to identify types of data and related formats that are expected to be used in the project.

2. **Ethical and Legal Compliance**: if either personal or sensitive data, or confidential data, are expected to be collected along the project, special attention should be given to ensure that legal and ethical issues are addressed in accordance with relevant regulations. This might include, ensure that data are saved on a secure platform, and that data are released (if at all) in an anonymous and secure way. More information available under [Licensing](##Licensing).

3. **Organizing and Documenting**: documents and data need to be organized in a way that can be understood and taken up by future colleagues or researchers interested in the existing data and work done. 
More information on how to best ensure so  are available under [Workflow](##Workflow), [Version](##Versioncontrol) [control](##Versioncontrol), and [Metadata](##Metadata).

4. **Storing and Sharing**: as briefly mentioned already in the previous step, data need to be stored on a safe platform, and possibly in a way that can be shared across collaborators and retrieved by the interested audiences. In order to do so, several practices exist, listed here by increasing level of complexity:

    a. *Manually creating a data package*, containing all the necessary information to ensure original data sources, data processing steps and contributors are clearly mentioned. This can be done via tools that provide easy-to-use templates to be filled in with pertainng information, such as the [Data Package Creator](<https://create.frictionlessdata.io/>).

    b. *Using the [Git](<https://git-scm.com/>) distributed [version](##Versioncontrol) [control](##Versioncontrol) system*, to help keeping track of changes and modifications from the original data to the final data format and structure needed for the intended research, and to document each and every step live while working on the data. The adoption of a distributed version control system that can be used across a large research group can be facilitated via dedicated online services, such as [GitHub](<https://github.com/>).

    c. *Generating a [workflow](##Workflow)*, to faclitate with simple steps and commands the reproduction of exactly the same data management process used produce the intended output. To generate a workflow, specific tools can be used such as [Snakemake](<https://snakemake.readthedocs.io/en/stable/>).

5. **Releasing**: once a new dataset or data package has been developed, it can be released and published online, and a related Digital Object Iidentifier (DOI) can be generated. This facilitates other people in citing and making use of it as new resource or to verify and reproduce existing research that already made use of the dataset.
In order to do so, several practices exist, listed here by increasing level of complexity:

    a. *Manually uploading data packages and providing related metadata on an online repository*, such as [Zenodo](<https://zenodo.org/>), following pre-defined templates.

    b. *Saving the data and documenting the related processing steps on a online development platform*, such as [GitHub](<https://github.com/>). In this case, metadata and documentation can be embedded along the process.

## Metadata
Metadata need to accompany the data to provide a basic set of information meant to support the retrievability of specific outputs and their correct citation. 
Metadata for data packages or datasets must include the following:

- [Attributions](##Attribution) (who the authors/contributors are, who did the work on collecting and compiling the data)
- [Licensing](##Licensing) clarifying under what conditions/rules the data can be used/modified/shared, etc.

In addition, metadata should also include the following:
- [Referencing](##Referencing) to source material, to provide information regarding where the data come from.
- Other relevant information, which might describe the methodology used for collecting the data, or the data format provided, or additional documentationi that might be considered relevant to understand the data and their potential use.

## Referencing
It is important to ensure that original data sources are properly documented and cited, in reference to the data used and processed in the develpent of a derivated data package or dataset used for research. 
This is needed to ensure that data are traceable.
Original data sources should be recorded and complete citations for each of the sources used in the generation of a data package or dataset should be listed in the related metadata.

## Attribution
To ensure all the contributors - i.e. people involved in collecting, processing, structuring and releaseing the data - are acknolwegded, the [CRediT author statement](<https://www.elsevier.com/authors/policies-and-guidelines/credit-author-statement>) can be used as reference.

For each contributors, the following basic personal information should be provided:
- Name and Surname
- Affiliation
- [ORCiD](<https://orcid.org/>) code
- Information about the contribution provided or the role covered by the person in question.

## Release
Releases can be created once a final version of the data as intended is produced and used for a specific application or as project output. 

Releases should be provided with a Digital Object Identifier (DOI), a complete Metadata file containing information regarding contributors and citation format.

## Licensing
It is important to ensure that data are always associated with a license, to clarify what are the legal restrictions (if present) in using, sharing and/or modifying existing data.
[CC BY licences](<https://creativecommons.org/about/cclicenses/>) are typically used for data licensing. Among CC BY licenses, there are more permissive and more restrictive licenses. Available on the Creative Commons website, [this tool](<https://creativecommons.org/choose/>) can help in choosing the right license for your data.

All licenses that apply to the original data sources should also be considered in selecting the license for the derivated data package or dataset released or shared as reseach output. This is to make sure the derivated work complies with any potential legal restriction imposed on the original data sources.

## Version control
Version control systems, such as [Git](<https://git-scm.com/>), and dedicated online services, such as [GitHub](<https://github.com/>), allow to record instances of text-based material and (e.g. data saved as .csv or .txt files) and then compare them to visualize differences and changes. They also enable collaborative workflows, thus facilitating researcher to work on the same data simultaneously, save and compare the separate progresses later on.

## Workflow
A workflow is defined by Goble et al. (2020)[ˆ2] as the description of “...the complex multi-step process to coordinate multiple tasks and their data dependencies”, where each step “represents the execution of a computational process”.

Workflows help with executing, reusing and reproducing, as well as reporting all steps needed to characterise the processing of data, thus supporting the use of data in the research context. 

The [Snakemake](<https://snakemake.readthedocs.io/en/stable/>) workflow management system can help with defining and automating a workflow. This allows other researchers or stakeholders to reproduce it with fewer, simpler steps that the ones required to initially define the workflow.



[ˆ1]: Viitanen E., 2022. Introduction to Research Data Management(RDM). *Aalto University*, Finland: Aalto. Available at: https://www.aalto.fi/en/services/training-in-research-data-management-and-open-science. (accessed: March 07, 2022) 

[ˆ2]: Goble, C., Cohen-Boulakia, S., Soiland-Reyes, S., Garijo, D., Gil, Y., Crusoe, M.R., Peters, K., Schober, D., 2020. FAIR Computational Workflows, *Data Intelligence*, vol. 2, no. 1–2, 1135 pp. 108–121. DOI: [10.1162/dint_a_00033](<https://direct.mit.edu/dint/article/2/1-2/108/10003/FAIR-Computational-Workflows>).